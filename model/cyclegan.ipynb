{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0b4e7f3195cf1e24ce8418331d59f43002c689dbd0c00d2a8952d60831217be82",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "b4e7f3195cf1e24ce8418331d59f43002c689dbd0c00d2a8952d60831217be82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Importing libraries and dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Importing the required datasets\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "from IPython.display import clear_output\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from EfficientNet import * #importing generator and discriminator models\n",
    "\n",
    "tfds.disable_progress_bar() #disabling the progress bar for training\n",
    "#load the tensorboard extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_train_pepsi = np.load('datasets/train_pepsi.npy')\n",
    "load_train_coke = np.load('datasets/train_coke.npy')\n",
    "load_test_pepsi = np.load('datasets/test_pepsi.npy')\n",
    "load_test_coke = np.load('datasets/test_coke.npy')\n",
    "\n",
    "load_train_pepsi_label = np.load('datasets/train_pepsi_label.npy')\n",
    "load_train_coke_label = np.load('datasets/train_coke_label.npy')\n",
    "load_test_pepsi_label = np.load('datasets/test_pepsi_label.npy')\n",
    "load_test_coke_label = np.load('datasets/test_coke_label.npy')\n",
    "\n",
    "\n",
    "#creating tensorflow datasets with numpy arrays\n",
    "trainB = tf.data.Dataset.from_tensor_slices((load_train_pepsi, load_train_pepsi_label))\n",
    "trainA = tf.data.Dataset.from_tensor_slices((load_train_coke, load_train_coke_label))\n",
    "testB = tf.data.Dataset.from_tensor_slices((load_test_pepsi, load_test_pepsi_label))\n",
    "testA = tf.data.Dataset.from_tensor_slices((load_test_coke, load_test_coke_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(load_train_pepsi.shape)\n",
    "print(load_train_pepsi_label.shape)\n",
    "\n",
    "print(load_train_pepsi_label[3])"
   ]
  },
  {
   "source": [
    "## Dataset preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the original image size - high resolution images can be used and cropped below\n",
    "orig_img_size = (286, 286)\n",
    "# Size of the random crops that will be applied to images\n",
    "input_img_size = (256, 256, 3)\n",
    "\n",
    "#setting batch size for the application of the preprocessing operations to the images in the dataset\n",
    "batch_size = 1\n",
    "\n",
    "def normalise_img(img):\n",
    "    img = tf.cast(img, dtype=tf.float32)\n",
    "    # Map values in the range [-1, 1]\n",
    "    return (img / 127.5) - 1.0\n",
    "\n",
    "\n",
    "def preprocess_train_image(img, label):\n",
    "    # Random flip\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    # Resize to the original size first\n",
    "    img = tf.image.resize(img, [*orig_img_size])\n",
    "    # Random crop to 256X256\n",
    "    img = tf.image.random_crop(img, size=[*input_img_size])\n",
    "    # Normalise the pixel values in the range [-1, 1]\n",
    "    img = normalise_img(img)\n",
    "    return img\n",
    "\n",
    "def preprocess_test_image(img, label):\n",
    "    # Only resizing and normalization for the test images.\n",
    "    img = tf.image.resize(img, [input_img_size[0], input_img_size[1]])\n",
    "    img = normalise_img(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "autotune = tf.data.experimental.AUTOTUNE\n",
    "# Apply the preprocessing operations to the training data\n",
    "trainA = (trainA.map(preprocess_train_image, num_parallel_calls=autotune).cache().shuffle(256).batch(batch_size))\n",
    "trainB = (trainB.map(preprocess_train_image, num_parallel_calls=autotune).cache().shuffle(256).batch(batch_size))\n",
    "\n",
    "# Apply the preprocessing operations to the test data\n",
    "testA = (testA.map(preprocess_test_image, num_parallel_calls=autotune).cache().shuffle(256).batch(batch_size))\n",
    "testB = (testB.map(preprocess_test_image, num_parallel_calls=autotune).cache().shuffle(256).batch(batch_size))"
   ]
  },
  {
   "source": [
    "## Model Compiling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "complete_efficientnet_generator() - Uses entire EfficientNet model with upsampling layers </br>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"RESNET BASELINE IMPLEMENTATION\"\"\"\n",
    "\n",
    "coke_to_pepsi_gen = resnet_generator(\n",
    "    filters=64,\n",
    "    num_resnet_blocks=9)\n",
    "pepsi_to_coke_gen = resnet_generator(\n",
    "    filters=64,\n",
    "    num_resnet_blocks=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"COMPLETE EFFICIENTNET IMPLEMENTATION\"\"\"\n",
    "\n",
    "coke_to_pepsi_gen = complete_efficientnet_generator()\n",
    "pepsi_to_coke_gen = complete_efficientnet_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"SPLIT EFFICIENTNET IMPLEMENTATION\"\"\"\n",
    "\n",
    "coke_to_pepsi_gen = efficientnet_generator(name='coke_to_pepsi_gen')\n",
    "pepsi_to_coke_gen = efficientnet_generator(name='pepsi_to_coke_gen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coke_discriminator = discriminator(name=\"coke_discriminator\")\n",
    "pepsi_discriminator = discriminator(name=\"pepsi_discriminator\")"
   ]
  },
  {
   "source": [
    "## Display examples"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coke_sample = next(itertools.cycle(trainA))\n",
    "pepsi_sample = next(itertools.cycle(trainB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "images = [coke_sample, pepsi_sample]\n",
    "title = ['Coke Sample', 'Pepsi Sample']\n",
    "\n",
    "for i in range(len(images)):\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    plt.title(title[i])\n",
    "    plt.imshow(images[i][0] * 0.5 + 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Is an image from Coke dataset?')\n",
    "plt.imshow(pepsi_discriminator(coke_sample)[0, ..., -1], cmap='RdBu_r')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Is an image from Pepsi dataset?')\n",
    "plt.imshow(coke_discriminator(pepsi_sample)[0, ..., -1], cmap='RdBu_r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Defining losses"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 10 #additional weight for cycle loss\n",
    "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real, generated):\n",
    "  real_loss = loss_obj(tf.ones_like(real), real)\n",
    "  \n",
    "  generated_loss = loss_obj(tf.zeros_like(generated), generated)\n",
    "\n",
    "  total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "  return total_disc_loss * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(generated):\n",
    "  return loss_obj(tf.ones_like(generated), generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(real_image, cycled_image):\n",
    "  loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
    "  \n",
    "  return LAMBDA * loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_loss(real_image, same_image):\n",
    "  loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
    "  return LAMBDA * 0.5 * loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coke_to_pepsi_gen_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5) #learning rate - 2e-4 or 0.0002\n",
    "pepsi_to_coke_gen_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "coke_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "pepsi_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "source": [
    "## Saving checkpoints"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initialisation of the checkpoint system that is used to save the models ouputs every 5 epochs. Checkpoints are also loaded for the model if they exist in the directory\"\"\"\n",
    "\n",
    "checkpoint_path = \"./checkpoints/train\" #Setting the checkpoint path\n",
    "\n",
    "ckpt = tf.train.Checkpoint(coke_to_pepsi_gen=coke_to_pepsi_gen,\n",
    "                           pepsi_to_coke_gen=pepsi_to_coke_gen,\n",
    "                           coke_discriminator=coke_discriminator,\n",
    "                           pepsi_discriminator=pepsi_discriminator,\n",
    "                           coke_to_pepsi_gen_optimizer=coke_to_pepsi_gen_optimizer,\n",
    "                           pepsi_to_coke_gen_optimizer=pepsi_to_coke_gen_optimizer,\n",
    "                           coke_discriminator_optimizer=coke_discriminator_optimizer,\n",
    "                           pepsi_discriminator_optimizer=pepsi_discriminator_optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint and display message .\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint has been restored')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_gen(model, test_input):\n",
    "  \"\"\"Returns the generated image after a sample has been through the generator model. This is used in the training loop to display the cyclic transformation\"\"\"\n",
    "\n",
    "  prediction = model(test_input)\n",
    "\n",
    "  display_list = [test_input[0], prediction[0]]\n",
    "  \n",
    "  return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(model, test_input):\n",
    "  \"\"\"Creates 1x2 plot containing the input image and that image after it has been through the generator specified\"\"\"\n",
    "\n",
    "  prediction = model(test_input)\n",
    "    \n",
    "  plt.figure(figsize=(12, 12))\n",
    "\n",
    "  display_list = [test_input[0], prediction[0]]\n",
    "  title = ['Input Image', 'Predicted Image']\n",
    "\n",
    "  for i in range(2):\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    plt.title(title[i])\n",
    "    # getting the pixel values between [0, 1] to plot.\n",
    "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "    plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Setting log directory and creating summary writer for tensorboard result tracking\"\"\"\n",
    "\n",
    "log_dir=\"logs/fit/\"\n",
    "summary_writer = tf.summary.create_file_writer(log_dir + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Paste into terminal to host tensorboard\"\"\"\n",
    "\n",
    "tensorboard --logdir logs/fit "
   ]
  },
  {
   "source": [
    "## Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def training_iteration(true_coke, true_pepsi, epoch):\n",
    "  # persistent is set to True because the tape is used more than once to calculate the gradients.\n",
    "  with tf.GradientTape(persistent=True) as tape:\n",
    "    # Generator G translates X -> Y\n",
    "    # Generator F translates Y -> X.\n",
    "    \n",
    "    \"\"\"TRAINING\"\"\"\n",
    "\n",
    "    fake_coke = pepsi_to_coke_gen(true_pepsi, training=True)\n",
    "    fake_pepsi = coke_to_pepsi_gen(true_coke, training=True)\n",
    "    \n",
    "    coke_cycle = pepsi_to_coke_gen(fake_pepsi, training=True)\n",
    "    pepsi_cycle = coke_to_pepsi_gen(fake_coke, training=True)\n",
    "\n",
    "    # coke_identity and pepsi_identity are used for identity loss.\n",
    "    coke_identity = pepsi_to_coke_gen(true_coke, training=True)\n",
    "    pepsi_identity = coke_to_pepsi_gen(true_pepsi, training=True)\n",
    "\n",
    "    real_coke_disc = coke_discriminator(true_coke, training=True)\n",
    "    real_pepsi_disc = pepsi_discriminator(true_pepsi, training=True)\n",
    "    \n",
    "    fake_coke_disc = coke_discriminator(fake_coke, training=True)\n",
    "    fake_pepsi_disc = pepsi_discriminator(fake_pepsi, training=True)\n",
    "\n",
    "    \"\"\"CALCULATING LOSSES\"\"\"\n",
    "\n",
    "    # calculate generator loss with binary crossentropy\n",
    "    coke_to_pepsi_gen_loss = generator_loss(fake_pepsi_disc)\n",
    "    pepsi_to_coke_gen_loss = generator_loss(fake_coke_disc)\n",
    "    \n",
    "    #calculating separate mean absolute errors for x and y images\n",
    "    coke_cycle_loss = mae(true_coke, coke_cycle)\n",
    "    pepsi_cycle_loss = mae(true_pepsi, pepsi_cycle)\n",
    "\n",
    "    #calculating the total mean absolute error (both x and y images)\n",
    "    complete_cycle = mae(true_coke, coke_cycle) + mae(true_pepsi, pepsi_cycle)\n",
    "\n",
    "    #total generator loss = adversarial loss + cycle loss\n",
    "    total_coke_to_pepsi_gen_loss = coke_to_pepsi_gen_loss + complete_cycle + identity_loss(true_pepsi, pepsi_identity)\n",
    "    total_pepsi_to_coke_gen_loss = pepsi_to_coke_gen_loss + complete_cycle + identity_loss(true_coke, coke_identity)\n",
    "\n",
    "    coke_disc_loss = discriminator_loss(real_coke_disc, fake_coke_disc)\n",
    "    pepsi_disc_loss = discriminator_loss(real_pepsi_disc, fake_pepsi_disc)\n",
    "\n",
    "  \"\"\"CALCULATING GRADIENT UPDATES\"\"\"\n",
    "\n",
    "  #gradients for generator and discriminator\n",
    "  coke_to_pepsi_gen_gradients = tape.gradient(total_coke_to_pepsi_gen_loss, coke_to_pepsi_gen.trainable_variables)\n",
    "  pepsi_to_coke_gen_gradients = tape.gradient(total_pepsi_to_coke_gen_loss, pepsi_to_coke_gen.trainable_variables)\n",
    "  coke_discriminator_gradients = tape.gradient(coke_disc_loss, coke_discriminator.trainable_variables)\n",
    "  pepsi_discriminator_gradients = tape.gradient(pepsi_disc_loss, pepsi_discriminator.trainable_variables)\n",
    "\n",
    "  \"\"\"APPLYING GRADIENT UPDATES\"\"\"\n",
    "  \n",
    "  coke_to_pepsi_gen_optimizer.apply_gradients(zip(coke_to_pepsi_gen_gradients, coke_to_pepsi_gen.trainable_variables))\n",
    "  pepsi_to_coke_gen_optimizer.apply_gradients(zip(pepsi_to_coke_gen_gradients, pepsi_to_coke_gen.trainable_variables))\n",
    "  coke_discriminator_optimizer.apply_gradients(zip(coke_discriminator_gradients, coke_discriminator.trainable_variables))\n",
    "  pepsi_discriminator_optimizer.apply_gradients(zip(pepsi_discriminator_gradients, pepsi_discriminator.trainable_variables))\n",
    "\n",
    "  \"\"\"WRITING LOSSES FOR TENSORBOARD\"\"\"\n",
    "  with summary_writer.as_default():\n",
    "    tf.summary.scalar('mae_x', coke_cycle_loss, step=epoch)\n",
    "    tf.summary.scalar('mae_y', pepsi_cycle_loss, step=epoch)\n",
    "    tf.summary.scalar('complete_cycle', complete_cycle, step=epoch)\n",
    "    tf.summary.scalar('total_coke_to_pepsi_gen_loss', total_coke_to_pepsi_gen_loss, step=epoch)\n",
    "    tf.summary.scalar('total_pepsi_to_coke_gen_loss', total_pepsi_to_coke_gen_loss, step=epoch)\n",
    "    tf.summary.scalar('coke_disc_loss', coke_disc_loss, step=epoch)\n",
    "    tf.summary.scalar('pepsi_disc_loss', pepsi_disc_loss, step=epoch)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs): #running the training loop for the number of epochs specified\n",
    "  start_time = time.time()\n",
    "  count = 0\n",
    "  for coke, pepsi in tf.data.Dataset.zip((trainA, trainB)): #zipping both training datasets together to loop over together\n",
    "    training_iteration(coke, pepsi, i) #for each epoch, run the training step on every image in the datasets\n",
    "    if count % 10 == 0:\n",
    "      print ('*', end=' ')\n",
    "    count += 1\n",
    "\n",
    "  clear_output(wait=True) #clearing output after each epoch to display new output\n",
    "  \n",
    "  # Using a consistent image (coke_sample) so that the progress of the model is clearly visible.\n",
    "  generate_images(coke_to_pepsi_gen, coke_sample) #display generated image for each epoch\n",
    "  \n",
    "  img = img_gen(coke_to_pepsi_gen, coke_sample) #assign generated image to img for each epoch\n",
    "  generate_images(pepsi_to_coke_gen, img) #display generated image back to original\n",
    "  \"\"\"\n",
    "  if (i + 1) % 5 == 0: #every 5 epochs save the checkpoint and display message stating this update\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print ('Saving checkpoint for epoch {} at {}'.format(i+1, ckpt_save_path))\n",
    "  \n",
    "  print ('Time taken for epoch {} is {} sec\\n'.format(i + 1, time.time()-start_time)) #print the time taken for each epoch. This is taking current time from start\n",
    "  \"\"\""
   ]
  },
  {
   "source": [
    "## Testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard dev upload --logdir logs/fit #add specific folder path "
   ]
  },
  {
   "source": [
    "### Final Tests\n",
    "\n",
    "All Tests - https://tensorboard.dev/experiment/m0AUwKOdR2GM7PS55AZBdw/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard dev delete --experiment_id #experiment_id_here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in testA.take(50):\n",
    "  generate_images(coke_to_pepsi_generator, img)\n",
    "  #generate_images(generator_f, translate_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}